{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "source_dir = os.path.join(current_dir)\n",
    "first_language = 'pl'\n",
    "second_language = 'en'\n",
    "\n",
    "source_dir = os.path.join(source_dir, f'{first_language}_{second_language}')\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    vocab_size = 8000,\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  starts = tf.fill([count,1], START)\n",
    "  ends = tf.fill([count,1], END)\n",
    "  return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "  return result\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text(encoding='utf-8').splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)\n",
    "\n",
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w',encoding='utf-8') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)\n",
    "      \n",
    "def make_own_tokenizers(train_first, train_secound, first_language='',secound_language=''):\n",
    "    first_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        train_first.batch(1000).prefetch(2),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "\n",
    "    secound_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        train_secound.batch(1000).prefetch(2),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "\n",
    "    write_vocab_file(os.path.join(source_dir,f'{first_language}_vocab.txt'), first_vocab)\n",
    "    write_vocab_file(os.path.join(source_dir,f'{secound_language}_vocab.txt'), secound_vocab)\n",
    "\n",
    "    first_tokenizer = text.BertTokenizer(os.path.join(source_dir,f'{first_language}_vocab.txt'), **bert_tokenizer_params)\n",
    "    secound_tokenizer = text.BertTokenizer(os.path.join(source_dir,f'{secound_language}_vocab.txt'), **bert_tokenizer_params)\n",
    "\n",
    "    return first_tokenizer, secound_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow_text.python.ops.bert_tokenizer.BertTokenizer at 0x190e1b917b0>,\n",
       " <tensorflow_text.python.ops.bert_tokenizer.BertTokenizer at 0x190dba8f6a0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(source_dir,f'{first_language}_{second_language}.tsv'), sep='\\t', header=None, names=[f'ID_{first_language}', first_language,f'ID_{second_language}', second_language]).loc[:, [first_language, second_language]]\n",
    "\n",
    "train_examples, val_examples = train_test_split(df, test_size=0.05, random_state=42)\n",
    "\n",
    "train_pl = train_examples[[first_language]]\n",
    "train_en = train_examples[[second_language]]\n",
    "\n",
    "train_pl = tf.data.Dataset.from_tensor_slices(train_pl.values)\n",
    "train_en = tf.data.Dataset.from_tensor_slices(train_en.values)\n",
    "\n",
    "make_own_tokenizers(train_pl, train_en, first_language=first_language, secound_language=second_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: d:\\Folder\\Studia\\VIII Semestr\\Sztuczne sieci neuronowe\\SSN_Example\\pl_en\\translate_pl_en_converter\\assets\n"
     ]
    }
   ],
   "source": [
    "model_name_pl_en = os.path.join(source_dir, f'translate_{first_language}_{second_language}_converter')\n",
    "\n",
    "tokenizers = tf.Module()\n",
    "tokenizers.pl= CustomTokenizer(reserved_tokens, os.path.join(source_dir,f'{first_language}_vocab.txt'))\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, os.path.join(source_dir,f'{second_language}_vocab.txt'))\n",
    "\n",
    "tf.saved_model.save(tokenizers, model_name_pl_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let s try something .\n",
      "sprobujmy cos .\n"
     ]
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name_pl_en)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()\n",
    "\n",
    "tokens = reloaded_tokenizers.en.tokenize(['Let s try something.'])\n",
    "\n",
    "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))\n",
    "\n",
    "reloaded_tokenizers.pl.get_vocab_size().numpy()\n",
    "\n",
    "tokens = reloaded_tokenizers.pl.tokenize(['Spróbujmy coś.'])\n",
    "\n",
    "round_trip = reloaded_tokenizers.pl.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
