{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1708,"status":"ok","timestamp":1683416860956,"user":{"displayName":"Tomasz Szkaradek","userId":"08379988213253496574"},"user_tz":-120},"id":"oCM0YJTKWpEZ","outputId":"573248cc-297c-4de7-95eb-4f1e8bd9095b"},"outputs":[],"source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","import sys\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow_datasets as tfds\n","import tensorflow_text as text\n","import tensorflow as tf\n","\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683416860956,"user":{"displayName":"Tomasz Szkaradek","userId":"08379988213253496574"},"user_tz":-120},"id":"lyV_HXn1WpEf"},"outputs":[],"source":["current_dir = os.getcwd()\n","source_dir = os.path.join(current_dir)\n","first_language = 'pt'\n","second_language = 'en'\n","ted_talks = True\n","\n","bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    vocab_size = 8000,\n","    reserved_tokens=reserved_tokens,\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    learn_params={},\n",")\n","\n","START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  return tf.concat([starts, ragged, ends], axis=1)\n","\n","def cleanup_text(reserved_tokens, token_txt):\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result\n","\n","class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text(encoding='utf-8').splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)\n","\n","\n","def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w',encoding='utf-8') as f:\n","    for token in vocab:\n","      print(token, file=f)\n","      \n","def make_own_tokenizers(train_first, train_secound, first_language='',secound_language=''):\n","    first_vocab = bert_vocab.bert_vocab_from_dataset(\n","        train_first.batch(1000).prefetch(2),\n","        **bert_vocab_args\n","    )\n","\n","    secound_vocab = bert_vocab.bert_vocab_from_dataset(\n","        train_secound.batch(1000).prefetch(2),\n","        **bert_vocab_args\n","    )\n","\n","    write_vocab_file(os.path.join(source_dir,f'{first_language}_vocab.txt'), first_vocab)\n","    write_vocab_file(os.path.join(source_dir,f'{secound_language}_vocab.txt'), secound_vocab)\n","\n","    first_tokenizer = text.BertTokenizer(os.path.join(source_dir,f'{first_language}_vocab.txt'), **bert_tokenizer_params)\n","    secound_tokenizer = text.BertTokenizer(os.path.join(source_dir,f'{secound_language}_vocab.txt'), **bert_tokenizer_params)\n","\n","    return first_tokenizer, secound_tokenizer"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":144189,"status":"ok","timestamp":1683417005143,"user":{"displayName":"Tomasz Szkaradek","userId":"08379988213253496574"},"user_tz":-120},"id":"oq_3_I4DWpEh"},"outputs":[],"source":["dictionary = os.path.join(source_dir, f'{first_language}_{second_language}')\n","if not os.path.exists(dictionary) or not os.path.isdir(dictionary):\n","    os.mkdir(dictionary)\n","source_dir = os.path.join(current_dir,  f'{first_language}_{second_language}')\n","\n","if ted_talks:\n","    examples, metadata = tfds.load(f'ted_hrlr_translate/{first_language}_to_{second_language}', with_info=True, as_supervised=True)\n","    train_examples, val_examples = examples['train'], examples['validation']\n","\n","    train_first = train_examples.map(lambda first, second: first)\n","    train_second = train_examples.map(lambda first, second: second)\n","\n","    make_own_tokenizers(train_first, train_second, first_language=first_language, secound_language=second_language)\n","else:\n","    df = pd.read_csv(os.path.join(source_dir,f'{first_language}_{second_language}.tsv'), sep='\\t', header=None, names=[f'ID_{second_language}', second_language, f'ID_{first_language}', first_language]).loc[:, [first_language, second_language]]\n","\n","    train_examples, val_examples = train_test_split(df, test_size=0.05, random_state=42)\n","\n","    train_first = train_examples[[first_language]]\n","    train_second = train_examples[[second_language]]\n","\n","    train_first = tf.data.Dataset.from_tensor_slices(train_first.values)\n","    train_second = tf.data.Dataset.from_tensor_slices(train_second.values)\n","\n","    make_own_tokenizers(train_first, train_second, first_language=first_language, secound_language=second_language)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4655,"status":"ok","timestamp":1683417009780,"user":{"displayName":"Tomasz Szkaradek","userId":"08379988213253496574"},"user_tz":-120},"id":"vxs_Ix6dWpEi"},"outputs":[],"source":["model_name = os.path.join(source_dir, f'translate_{first_language}_{second_language}_converter')\n","\n","tokenizers = tf.Module()\n","tokenizers.pt= CustomTokenizer(reserved_tokens, os.path.join(source_dir,f'{first_language}_vocab.txt'))\n","tokenizers.en = CustomTokenizer(reserved_tokens, os.path.join(source_dir,f'{second_language}_vocab.txt'))\n","\n","tf.saved_model.save(tokenizers, model_name)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"elapsed":659,"status":"error","timestamp":1683417010425,"user":{"displayName":"Tomasz Szkaradek","userId":"08379988213253496574"},"user_tz":-120},"id":"x2GWjoZYWpEj","outputId":"e9320da0-4fd1-4193-8d8a-23aa5ec5ac2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["eu li sobre triceratops na enciclopedia .\n","i read about triceratops in the encyclopedia .\n"]}],"source":["reloaded_tokenizers = tf.saved_model.load(model_name)\n","reloaded_tokenizers.pt.get_vocab_size().numpy()\n","\n","tokens = reloaded_tokenizers.pt.tokenize(['Eu li sobre triceratops na enciclop√©dia.'])\n","\n","round_trip = reloaded_tokenizers.pt.detokenize(tokens)\n","\n","print(round_trip.numpy()[0].decode('utf-8'))\n","\n","reloaded_tokenizers.en.get_vocab_size().numpy()\n","\n","tokens = reloaded_tokenizers.en.tokenize(['I read about triceratops in the encyclopedia.'])\n","\n","round_trip = reloaded_tokenizers.en.detokenize(tokens)\n","\n","print(round_trip.numpy()[0].decode('utf-8'))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
